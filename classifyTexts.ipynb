{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pipeline for classifiying texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To do: installation instructions_\n",
    "\n",
    "* numpy\n",
    "* scikit learn\n",
    "* joblib\n",
    "* python-joblib\n",
    "* scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier\n",
    "plt.rcParams['figure.figsize'] = (18, 5)\n",
    "plt.rcParams['font.family'] = 'sans-serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier expects input text files of containing:  \n",
    "`sentence id[tab]sentence[tab]None`  \n",
    "etc.\n",
    "\n",
    "The sentence should be tokenized, and tokens should be separated by a space.\n",
    "\n",
    "It is best to have a single file for each text for which labels should be predicted.\n",
    "\n",
    "The text files should be put together in a single directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To do: make notebook with pipeline for converting text files to files that can be used for prediction._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# path to the input data\n",
    "data_dir = '/home/jvdzwaan/data/embem/txt/corpus_big-for_prediction/'\n",
    "\n",
    "# specify the path where output should be written\n",
    "out_dir = '/home/jvdzwaan/data/tmp/bla'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# classifier file\n",
    "classifier = '/home/jvdzwaan/data/classifier/classifier.pkl'\n",
    "\n",
    "# train file\n",
    "train_file = '/home/jvdzwaan/data/embem_ml/multilabel-normalized/all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load utility functionality\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def load_data(data_file):\n",
    "    data = [ln.rsplit(None, 1) for ln in open(data_file)]\n",
    "\n",
    "    X_data, Y_data = zip(*data)\n",
    "\n",
    "    return X_data, Y_data\n",
    "\n",
    "\n",
    "def get_data(train_file, test_file):\n",
    "    X_train, Y_train = load_data(train_file)\n",
    "    X_train = [ln.split('\\t')[1] for ln in X_train]\n",
    "    X_test, Y_test = load_data(test_file)\n",
    "    X_test = [ln.split('\\t')[1] for ln in X_test]\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y_train = [set(s.split('_')) - {'None'} for s in Y_train]\n",
    "    Y_test = [set(s.split('_')) - {'None'} for s in Y_test]\n",
    "    Y_train = mlb.fit_transform(Y_train)\n",
    "    Y_test = mlb.transform(Y_test)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, mlb.classes_\n",
    "\n",
    "\n",
    "def split(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 149 corn001dood01.txt\n",
      "2 of 149 plui001verl01.txt\n",
      "3 of 149 bred001gria01.txt\n",
      "4 of 149 koni001twee01.txt\n",
      "5 of 149 vos_002aran03.txt\n",
      "6 of 149 stee033beon01.txt\n",
      "7 of 149 feit007patr01.txt\n",
      "8 of 149 hoev003rech01.txt\n",
      "9 of 149 zand008deme01.txt\n",
      "10 of 149 raci001hest01.txt\n",
      "11 of 149 aren001joan01.txt\n",
      "12 of 149 wild007swer01.txt\n",
      "13 of 149 hoev003isab01.txt\n",
      "14 of 149 vond001pete01.txt\n",
      "15 of 149 moli015bela01.txt\n",
      "16 of 149 bran002vein02.txt\n",
      "17 of 149 gaet001ontm01.txt\n",
      "18 of 149 corn001cid_02.txt\n",
      "19 of 149 haps002soph02.txt\n",
      "20 of 149 scha003voor01.txt\n",
      "21 of 149 boon045leid02.txt\n",
      "22 of 149 bidl001fabi01.txt\n",
      "23 of 149 lang020gava03.txt\n",
      "24 of 149 moli015scho02.txt\n",
      "25 of 149 stey002geve01.txt\n",
      "26 of 149 zeer001eers01.txt\n",
      "27 of 149 bidl001kare02.txt\n",
      "28 of 149 wild007abra01.txt\n",
      "29 of 149 hoev003dood02.txt\n",
      "30 of 149 elst004jano01.txt\n",
      "31 of 149 bidl001vert01.txt\n",
      "32 of 149 foss005manl01.txt\n",
      "33 of 149 tijs003merk01.txt\n",
      "34 of 149 bern001athi01.txt\n",
      "35 of 149 croi003meid01.txt\n",
      "36 of 149 corn001hora02.txt\n",
      "37 of 149 vinc001hoog01.txt\n",
      "38 of 149 brui008verh02.txt\n",
      "39 of 149 chap010mark02.txt\n",
      "40 of 149 cost001rako01.txt\n",
      "41 of 149 quin031spoo02.txt\n",
      "42 of 149 kalb001muli01.txt\n",
      "43 of 149 vond001hier01.txt\n",
      "44 of 149 corn001sert01.txt\n",
      "45 of 149 wits004alle01.txt\n",
      "46 of 149 vond001salm02.txt\n",
      "47 of 149 sauv003maho01.txt\n",
      "48 of 149 buys001brui05.txt\n",
      "49 of 149 bont001bele02.txt\n",
      "50 of 149 bidl001zege01.txt\n",
      "51 of 149 asse001open01.txt\n",
      "52 of 149 expe001suri01.txt\n",
      "53 of 149 sain011orak01.txt\n",
      "54 of 149 bred001ange01.txt\n",
      "55 of 149 marr001marc01.txt\n",
      "56 of 149 lope001joan01.txt\n",
      "57 of 149 gilb007lief01.txt\n",
      "58 of 149 vinc001list01.txt\n",
      "59 of 149 gres007edua01.txt\n",
      "60 of 149 hoof009blyd01.txt\n",
      "61 of 149 nore003list01.txt\n",
      "62 of 149 corn001andr03.txt\n",
      "63 of 149 vond001gysb01.txt\n",
      "64 of 149 scha003bela01.txt\n",
      "65 of 149 scha003leve01.txt\n",
      "66 of 149 stij003anme01.txt\n",
      "67 of 149 _par004paro01.txt\n",
      "68 of 149 hoof001thes01.txt\n",
      "69 of 149 grae002alci01.txt\n",
      "70 of 149 nooz002kluc01.txt\n",
      "71 of 149 rulo001inwy01.txt\n",
      "72 of 149 rode001aure01.txt\n",
      "73 of 149 vinc001leev01.txt\n",
      "74 of 149 nuyt001adme01.txt\n",
      "75 of 149 bidl001oper01.txt\n",
      "76 of 149 foss005poly02.txt\n",
      "77 of 149 bred001schy01.txt\n",
      "78 of 149 bred001luce01.txt\n",
      "79 of 149 bred001rodd01.txt\n",
      "80 of 149 plui001verl02.txt\n",
      "81 of 149 moli015schy01.txt\n",
      "82 of 149 nooz003gete02.txt\n",
      "83 of 149 moli015fiel02.txt\n",
      "84 of 149 asse001kwak01.txt\n",
      "85 of 149 tijs003wind01.txt\n",
      "86 of 149 ling001apol01.txt\n",
      "87 of 149 lope001dull01.txt\n",
      "88 of 149 doms001besc03.txt\n",
      "89 of 149 stee033bele01.txt\n",
      "90 of 149 quin031agri02.txt\n",
      "91 of 149 raci001ifig01.txt\n",
      "92 of 149 berg038donj02.txt\n",
      "93 of 149 swae001verh01.txt\n",
      "94 of 149 blan049juff01.txt\n",
      "95 of 149 anto001gely02.txt\n",
      "96 of 149 vos_002mede03.txt\n",
      "97 of 149 huyg001trij01.txt\n",
      "98 of 149 fres003inen01.txt\n",
      "99 of 149 noms001iema01.txt\n",
      "100 of 149 ouda001haag01.txt\n",
      "101 of 149 ling001sard01.txt\n",
      "102 of 149 corn104verm01.txt\n",
      "103 of 149 brui008gron01.txt\n",
      "104 of 149 ling001cleo01.txt\n",
      "105 of 149 hoev003dood01.txt\n",
      "106 of 149 vos_001iema03.txt\n",
      "107 of 149 sche001mele01.txt\n",
      "108 of 149 mol_014bedr01.txt\n",
      "109 of 149 vos_001kluc05.txt\n",
      "110 of 149 barb020tomy01.txt\n",
      "111 of 149 lesa002kris02.txt\n",
      "112 of 149 vond001luci01.txt\n",
      "113 of 149 corn001poli02.txt\n",
      "114 of 149 leeu004tove02.txt\n",
      "115 of 149 moli015lubb02.txt\n",
      "116 of 149 asse001gusm02.txt\n",
      "117 of 149 quin031mede01.txt\n",
      "118 of 149 vond001elek01.txt\n",
      "119 of 149 raci001mith01.txt\n",
      "120 of 149 rijn001oude01.txt\n",
      "121 of 149 rotg001lstr01.txt\n",
      "122 of 149 moli015inge01.txt\n",
      "123 of 149 corn001cinn01.txt\n",
      "124 of 149 quin031wanh02.txt\n",
      "125 of 149 bred006kris01.txt\n",
      "126 of 149 vond001maeg04.txt\n",
      "127 of 149 _qua002quae02.txt\n",
      "128 of 149 lope001gedw04.txt\n",
      "129 of 149 mira010verw02.txt\n",
      "130 of 149 lope001bekl02.txt\n",
      "131 of 149 croi003gewa01.txt\n",
      "132 of 149 blan049aben01.txt\n",
      "133 of 149 moli015burg01.txt\n",
      "134 of 149 six_001mede01.txt\n",
      "135 of 149 zasy001borg01.txt\n",
      "136 of 149 vos_001kluc04.txt\n",
      "137 of 149 vond001pasc02.txt\n",
      "138 of 149 peys001tove01.txt\n",
      "139 of 149 rotr001groo02.txt\n",
      "140 of 149 quin031toon01.txt\n",
      "141 of 149 cost001teeu01.txt\n",
      "142 of 149 stee033andr01.txt\n",
      "143 of 149 bred001dage01.txt\n",
      "144 of 149 boel009bedr01.txt\n",
      "145 of 149 moli015verw02.txt\n",
      "146 of 149 bred001stom01.txt\n",
      "147 of 149 smid001konr02.txt\n",
      "148 of 149 _par003paro01.txt\n",
      "149 of 149 rijk001atha01.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import codecs\n",
    "\n",
    "# load classifier\n",
    "clf = joblib.load(classifier)\n",
    "\n",
    "text_files = [fi for fi in os.listdir(data_dir) if fi.endswith('.txt')]\n",
    "for i, text_file in enumerate(text_files):\n",
    "    in_file = os.path.join(data_dir, text_file)\n",
    "    print('{} of {}'.format(i+1, len(text_files)), text_file)\n",
    "\n",
    "    # load data\n",
    "    X_train, X_data, Y_train, Y_data, classes_ = get_data(train_file, in_file)\n",
    "\n",
    "    # classifiy\n",
    "    pred = clf.predict(X_data)\n",
    "\n",
    "    # save results\n",
    "    out_file = os.path.join(out_dir, text_file)\n",
    "\n",
    "    X_data_with_ids, Y_data = load_data(in_file)\n",
    "\n",
    "    with codecs.open(out_file, 'wb', 'utf8') as f:\n",
    "        for x, y in zip(X_data_with_ids, pred):\n",
    "            f.write(u'{}\\t{}\\n'.format(x.decode('utf8'),\n",
    "                                       '_'.join(classes_[y]) or 'None'))\n",
    "\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next step is to look at the results!\n",
    "\n",
    "_To do: pipeline for showing/visualizing results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
