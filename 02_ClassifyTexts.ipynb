{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pipeline for classifiying texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier expects input text files of containing:  \n",
    "`sentence id[tab]sentence[tab]None`  \n",
    "etc. \n",
    "\n",
    "The sentences should be tokenized, and tokens should be separated by a space.\n",
    "\n",
    "It is best to have a single file for each text for which labels should be predicted.\n",
    "\n",
    "The text files should be put together in a single directory.\n",
    "\n",
    "Use notebook [00_CreateClassifiers](00_CreateClassifiers.ipynb) to create the classifier and [01_CreateDataForPrediction](01_CreateDataForPrediction.ipynb) to generate data in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embem_data_dir = '/home/jvdzwaan/data/embem/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Annotation\n",
    "\n",
    "# path to the input data\n",
    "data_dir = os.path.join(embem_data_dir, 'txt/annotation-for_prediction-normalized/')\n",
    "\n",
    "# specify the path where output should be written\n",
    "out_dir = '~/tmp/annotation-predicted-heem-normalized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Corpus big\n",
    "\n",
    "# path to the input data\n",
    "data_dir = os.path.join(embem_data_dir, 'txt/corpus_big-for_prediction-normalized/')\n",
    "\n",
    "# specify the path where output should be written\n",
    "out_dir = '~/tmp/corpus_big-predicted-heem-normalized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ceneton data\n",
    "\n",
    "# path to the input data\n",
    "data_dir = os.path.join(embem_data_dir, 'txt/ceneton-for_prediction-normalized/')\n",
    "\n",
    "# specify the path where output should be written\n",
    "out_dir = '~/tmp/ceneton-predicted-heem-normalized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EDBO data\n",
    "\n",
    "# path to the input data\n",
    "data_dir = os.path.join(embem_data_dir, 'txt/edbo-for_prediction-normalized/')\n",
    "\n",
    "# specify the path where output should be written\n",
    "out_dir = '~/tmp/edbo-predicted-heem-normalized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# classifier file\n",
    "classifier = '/home/jvdzwaan/data/classifier/classifier.pkl'\n",
    "\n",
    "# train file\n",
    "train_file = os.path.join(embem_data_dir, 'ml/all_spellingnormalized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 of 67) Ff48fdcd8588e.txt\n",
      "(2 of 67) F1bdd71564d85.txt\n",
      "(3 of 67) F168fa8f9842e.txt\n",
      "(4 of 67) F5a1168654ba9.txt\n",
      "(5 of 67) Feb376834078a.txt\n",
      "(6 of 67) F934371ae3480.txt\n",
      "(7 of 67) Fde99457f05b2.txt\n",
      "(8 of 67) Ff1277be87709.txt\n",
      "(9 of 67) F2def6fc1991d.txt\n",
      "(10 of 67) Fafac452c47f2.txt\n",
      "(11 of 67) F1fc4e2f26f83.txt\n",
      "(12 of 67) F00e07fe03042.txt\n",
      "(13 of 67) Fe2261ffc9608.txt\n",
      "(14 of 67) Fdec68314024d.txt\n",
      "(15 of 67) Fde9ccd6aba49.txt\n",
      "(16 of 67) Ff181f3aadced.txt\n",
      "(17 of 67) F5758b6f36ced.txt\n",
      "(18 of 67) Fc2ab541f9310.txt\n",
      "(19 of 67) F16f680783b48.txt\n",
      "(20 of 67) F0c180c461248.txt\n",
      "(21 of 67) F237437d6c466.txt\n",
      "(22 of 67) F1d3f48d0974d.txt\n",
      "(23 of 67) F9d26211403b2.txt\n",
      "(24 of 67) Fdf230a240d87.txt\n",
      "(25 of 67) Fc053722ef59a.txt\n",
      "(26 of 67) F572e994794fd.txt\n",
      "(27 of 67) F88892c3a86b1.txt\n",
      "(28 of 67) F4b5458704e16.txt\n",
      "(29 of 67) F8888dddfd2dc.txt\n",
      "(30 of 67) Fa252607e6eef.txt\n",
      "(31 of 67) Fbfac05bb416b.txt\n",
      "(32 of 67) Fba52678c1f75.txt\n",
      "(33 of 67) Fe0a7e4c6ab5f.txt\n",
      "(34 of 67) F78efb1028513.txt\n",
      "(35 of 67) Ff899faf1f27a.txt\n",
      "(36 of 67) F3638a5322877.txt\n",
      "(37 of 67) F62f2a9465577.txt\n",
      "(38 of 67) Fd52ac420d560.txt\n",
      "(39 of 67) F6e2ee00f7923.txt\n",
      "(40 of 67) Fe5ba27c960ef.txt\n",
      "(41 of 67) Ffc4c9ad7e26b.txt\n",
      "(42 of 67) Fc3b2e33a908e.txt\n",
      "(43 of 67) Fc88c1f5207f6.txt\n",
      "(44 of 67) F5621245aa9ff.txt\n",
      "(45 of 67) F9f53f91c8b33.txt\n",
      "(46 of 67) F1efc22bbaaef.txt\n",
      "(47 of 67) F25bc6d62c587.txt\n",
      "(48 of 67) Fea3a73500ba3.txt\n",
      "(49 of 67) F8d8b1f0c2db8.txt\n",
      "(50 of 67) F2956ed0af5d1.txt\n",
      "(51 of 67) F5dce36e5dbe0.txt\n",
      "(52 of 67) F2dd6e857dc91.txt\n",
      "(53 of 67) Ff019d344615a.txt\n",
      "(54 of 67) Fa105fea365b2.txt\n",
      "(55 of 67) F84a0273f4636.txt\n",
      "(56 of 67) F2326ec0fa910.txt\n",
      "(57 of 67) Ffd48e027871e.txt\n",
      "(58 of 67) Fcc6232440c73.txt\n",
      "(59 of 67) F5386951ff0d9.txt\n",
      "(60 of 67) F3d0b532583ca.txt\n",
      "(61 of 67) F584bfe7ebaa7.txt\n",
      "(62 of 67) F99490d2f3f26.txt\n",
      "(63 of 67) Fccb23dad4e3b.txt\n",
      "(64 of 67) F508fe5d4fd32.txt\n",
      "(65 of 67) Fb167a7fd9cf3.txt\n",
      "(66 of 67) F75ad7db0a874.txt\n",
      "(67 of 67) Ffac41d4f3021.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import codecs\n",
    "from utils import get_data, load_data\n",
    "\n",
    "# load classifier\n",
    "clf = joblib.load(classifier)\n",
    "\n",
    "text_files = [fi for fi in os.listdir(data_dir) if fi.endswith('.txt')]\n",
    "for i, text_file in enumerate(text_files):\n",
    "    in_file = os.path.join(data_dir, text_file)\n",
    "    print('({} of {}) {}'.format(i+1, len(text_files), text_file))\n",
    "\n",
    "    # load data\n",
    "    X_train, X_data, Y_train, Y_data, classes_ = get_data(train_file, in_file)\n",
    "\n",
    "    # classifiy\n",
    "    pred = clf.predict(X_data)\n",
    "\n",
    "    # save results\n",
    "    out_file = os.path.join(out_dir, text_file)\n",
    "\n",
    "    X_data_with_ids, Y_data = load_data(in_file)\n",
    "\n",
    "    with codecs.open(out_file, 'wb', 'utf8') as f:\n",
    "        for x, y in zip(X_data_with_ids, pred):\n",
    "            f.write(u'{}\\t{}\\n'.format(x.decode('utf8'),\n",
    "                                       '_'.join(classes_[y]) or 'None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 of 29) vond001gysb04.txt\n",
      "(2 of 29) ross006zing01.txt\n",
      "(3 of 29) huyd001achi01.txt\n",
      "(4 of 29) hoof001gran01.txt\n",
      "(5 of 29) stee033adag01.txt\n",
      "(6 of 29) rivi001jeug01.txt\n",
      "(7 of 29) fres003pefr01.txt\n",
      "(8 of 29) bidl001nede01.txt\n",
      "(9 of 29) hare003agon01.txt\n",
      "(10 of 29) alew001puit01.txt\n",
      "(11 of 29) hoof001achi01.txt\n",
      "(12 of 29) lijn002vlug01.txt\n",
      "(13 of 29) bred001moor01.txt\n",
      "(14 of 29) stee033tham01.txt\n",
      "(15 of 29) alew001besl01.txt\n",
      "(16 of 29) vinc001pefr02.txt\n",
      "(17 of 29) bren001scha01.txt\n",
      "(18 of 29) lang020chph01.txt\n",
      "(19 of 29) bren001goud01.txt\n",
      "(20 of 29) meij001verl01.txt\n",
      "(21 of 29) vond001jose05.txt\n",
      "(22 of 29) vond001pala01.txt\n",
      "(23 of 29) rivi001vero01.txt\n",
      "(24 of 29) pels001verw02.txt\n",
      "(25 of 29) focq001mini02.txt\n",
      "(26 of 29) noms001mich01.txt\n",
      "(27 of 29) weye002holl01.txt\n",
      "(28 of 29) vos_002kluc01.txt\n",
      "(29 of 29) ling001ontd01.txt\n"
     ]
    }
   ],
   "source": [
    "# make unnormalized version of predicted labels (needed before expanding body part labels)\n",
    "\n",
    "%run merge_data_and_labels.py /home/jvdzwaan/data/embem/txt/annotation-predicted-heem-normalized/ /home/jvdzwaan/data/embem/txt/annotation-for_prediction/ /home/jvdzwaan/data/embem/txt/annotation-predicted-heem\n",
    "#%run merge_data_and_labels.py /home/jvdzwaan/data/embem/txt/corpus_big-predicted-heem-normalized/ /home/jvdzwaan/data/embem/txt/corpus_big-for_prediction/ /home/jvdzwaan/data/embem/txt/corpus_big-predicted-heem\n",
    "#%run merge_data_and_labels.py /home/jvdzwaan/data/embem/txt/ceneton-predicted-heem-normalized/ /home/jvdzwaan/data/embem/txt/ceneton-for_prediction/ /home/jvdzwaan/data/embem/txt/ceneton-predicted-heem\n",
    "#%run merge_data_and_labels.py /home/jvdzwaan/data/embem/txt/edbo-predicted-heem-normalized/ /home/jvdzwaan/data/embem/txt/edbo-for_prediction/ /home/jvdzwaan/data/embem/txt/edbo-predicted-heem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored: rose-kaken (cheeks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classify_body_parts.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if w in word2cat.keys():\n"
     ]
    }
   ],
   "source": [
    "# Expand body parts\n",
    "\n",
    "%run classify_body_parts.py /home/jvdzwaan/data/embem/dict/body_part_mapping.json /home/jvdzwaan/data/embem/txt/annotation-predicted-heem/ /home/jvdzwaan/data/embem/txt/annotation-predicted-heem-expanded_body_parts  /home/jvdzwaan/data/embem/dict/annotation_heem_expanded_body_parts.csv\n",
    "#%run classify_body_parts.py /home/jvdzwaan/data/embem/dict/body_part_mapping.json /home/jvdzwaan/data/embem/txt/corpus_big-predicted-heem/ /home/jvdzwaan/data/embem/txt/corpus_big-predicted-heem-expanded_body_parts  /home/jvdzwaan/data/embem/dict/corpus_big_heem_expanded_body_parts.csv\n",
    "#%run classify_body_parts.py /home/jvdzwaan/data/embem/dict/body_part_mapping.json /home/jvdzwaan/data/embem/txt/ceneton-predicted-heem/ /home/jvdzwaan/data/embem/txt/ceneton-predicted-heem-expanded_body_parts  /home/jvdzwaan/data/embem/dict/ceneton_heem_expanded_body_parts.csv\n",
    "#%run classify_body_parts.py /home/jvdzwaan/data/embem/dict/body_part_mapping.json /home/jvdzwaan/data/embem/txt/edbo-predicted-heem/ /home/jvdzwaan/data/embem/txt/edbo-predicted-heem-expanded_body_parts  /home/jvdzwaan/data/embem/dict/edbo_heem_expanded_body_parts.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next step is to look at the results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
